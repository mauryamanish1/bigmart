{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to build a predictive model and predict the sales of each product at a particular outlet.\n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and outlets which play a key role in increasing sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission file format\n",
    "\n",
    "<!-- Variable\tDescription\n",
    "Item_Identifier\tUnique product ID\n",
    "Outlet_Identifier\tUnique store ID\n",
    "Item_Outlet_Sales\tSales of the product in the particular store. This is the outcome variable to be predicted. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission file format\n",
    "\n",
    "# Variable\tDescription\n",
    "# Item_Identifier\tUnique product ID\n",
    "# Outlet_Identifier\tUnique store ID\n",
    "# Item_Outlet_Sales\tSales of the product in the particular store. This is the outcome variable to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/Users/manishmaurya/Downloads/train_v9rqX0R.csv')\n",
    "test_data=pd.read_csv('/Users/manishmaurya/Downloads/test_AbJTz2l.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Item_Fat_Content'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}\n",
    "\n",
    "# Replace the values in the 'Item_Fat_Content' column\n",
    "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace(replacements)\n",
    "test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace(replacements)\n",
    "data['Outlet_Age']=data['Outlet_Establishment_Year'].max()-data['Outlet_Establishment_Year']+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to fill NaN values in Outlet_Size\n",
    "fill_values = {\n",
    "    'Grocery Store': 'Small',\n",
    "    'Supermarket Type2': 'Medium',\n",
    "    'Supermarket Type3': 'Medium'\n",
    "}\n",
    "\n",
    "# Fill NaN values in Outlet_Size based on Outlet_Type\n",
    "data2['Outlet_Size'] = data2.apply(\n",
    "    lambda row: fill_values.get(row['Outlet_Type'], row['Outlet_Size']) if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "replace_type_values= {\n",
    "    'Supermarket Type1':1, \n",
    "    'Supermarket Type2':2, \n",
    "    'Grocery Store':0,\n",
    "    'Supermarket Type3':3\n",
    "}\n",
    "data2['Outlet_Type']=data2['Outlet_Type'].replace(replace_type_values)\n",
    "\n",
    "\n",
    "replace_Size_values= {\n",
    "    'High':3, \n",
    "    'Medium':2, \n",
    "    'Small':1,\n",
    "    \n",
    "}\n",
    "data2['Outlet_Size']=data2['Outlet_Size'].replace(replace_Size_values)\n",
    "\n",
    "# Dictionary to replace values in Outlet_Location_Type\n",
    "replace_loc_values = {\n",
    "    'Tier 1': 1,\n",
    "    'Tier 2': 2,\n",
    "    'Tier 3': 3\n",
    "}\n",
    "\n",
    "# Replace values in Outlet_Location_Type based on the dictionary\n",
    "data2['Outlet_Location_Type'] = data2['Outlet_Location_Type'].replace(replace_loc_values)\n",
    "\n",
    "# Replace Item_Fat_Content values\n",
    "data2.loc[data2['Item_Type'] == 'Household', 'Item_Fat_Content'] = 0\n",
    "\n",
    "data2['Item_Fat_Content'] = data2['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n",
    "\n",
    "\n",
    "data2['feat1']=data2['Item_MRP'] * data2['Outlet_Type']\n",
    "data2['feat2']=data2['Item_MRP'] **2\n",
    "\n",
    "\n",
    "# test_data['qty']=data2['Item_Outlet_Sales']/data2['Item_MRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2.pivot_table(index='Item_Type', columns='Outlet_Type', values='Item_Identifier', aggfunc='count').plot(figsize=(16,4),kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2.pivot_table(index='Item_Type', columns='Outlet_Location_Type', values='Item_Identifier', aggfunc='count').plot(figsize=(16,4),kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Item_Fat_Content'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for item wise mrp to sales correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data2['Item_Type'].unique():\n",
    "    sns.relplot(data2.loc[data2['Item_Type']==x],x='Item_MRP',y='Item_Outlet_Sales',hue='Outlet_Type')\n",
    "    plt.title(x.strip())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat=['Item_Weight','Item_Identifier',\t'Outlet_Identifier','Item_Fat_Content',\t'Item_Visibility',\t'Item_Type'\t,'Item_MRP',\t'Outlet_Identifier',\t\t'Outlet_Size',\t'Outlet_Location_Type',\t'Outlet_Type',\t'Item_Outlet_Sales'\t,'Outlet_Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.linear_model import Ridge  # Using Ridge Regression as a good ML model\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# # Data Preprocessing\n",
    "# # 1. Handle missing values\n",
    "# imputer_num = SimpleImputer(strategy='mean')\n",
    "# imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# data2['Item_Weight'] = imputer_num.fit_transform(data2[['Item_Weight']])\n",
    "# data2['Outlet_Size'] = imputer_cat.fit_transform(data2[['Outlet_Size']])\n",
    "\n",
    "# # 2. Feature engineering (example: Item_Visibility = 0 is noise)\n",
    "# data2['Item_Visibility'] = data2['Item_Visibility'].replace(0, data2['Item_Visibility'].mean())\n",
    "\n",
    "# # 3. Feature selection (if needed)\n",
    "\n",
    "\n",
    "# # 4. Separate features and target\n",
    "# X = data2.drop('Item_Outlet_Sales',axis=1)\n",
    "# y = data2['Item_Outlet_Sales']\n",
    "\n",
    "# # 5. Identify numerical and categorical columns\n",
    "# numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# # 6. Create transformers for numerical and categorical features\n",
    "# numerical_transformer = MinMaxScaler()\n",
    "# categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# # 7. Combine transformers using ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_cols),\n",
    "#         ('cat', categorical_transformer, categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # 8. Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 9. Apply preprocessing to training and testing data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "# X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# # Model Training (Ridge Regression)\n",
    "# model = Ridge(alpha=1.0) # You can tune alpha\n",
    "# model.fit(X_train_processed, y_train)\n",
    "\n",
    "# # Model Evaluation\n",
    "# y_pred = model.predict(X_test_processed)\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor  # Using RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Transformations on Train and Test Data (Consistent Transformations)\n",
    "# replacements = {'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'}\n",
    "# data2['Item_Fat_Content'] = data2['Item_Fat_Content'].replace(replacements)\n",
    "# test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace(replacements)\n",
    "# data2['Outlet_Age'] = data2['Outlet_Establishment_Year'].max() - data2['Outlet_Establishment_Year'] + 1\n",
    "# test_data['Outlet_Age'] = test_data['Outlet_Establishment_Year'].max() - test_data['Outlet_Establishment_Year'] + 1\n",
    "# fill_values = {'Grocery Store': 'Small', 'Supermarket Type2': 'Medium', 'Supermarket Type3': 'Medium'}\n",
    "# data2['Outlet_Size'] = data2.apply(lambda row: fill_values.get(row['Outlet_Type'], row['Outlet_Size']) if pd.isna(row['Outlet_Size']) else row['Outlet_Size'], axis=1)\n",
    "# test_data['Outlet_Size'] = test_data.apply(lambda row: fill_values.get(row['Outlet_Type'], row['Outlet_Size']) if pd.isna(row['Outlet_Size']) else row['Outlet_Size'], axis=1)\n",
    "# replace_type_values = {'Supermarket Type1': 1, 'Supermarket Type2': 2, 'Grocery Store': 0, 'Supermarket Type3': 3}\n",
    "# data2['Outlet_Type'] = data2['Outlet_Type'].replace(replace_type_values)\n",
    "# test_data['Outlet_Type'] = test_data['Outlet_Type'].replace(replace_type_values)\n",
    "# replace_Size_values = {'High': 3, 'Medium': 2, 'Small': 1}\n",
    "# data2['Outlet_Size'] = data2['Outlet_Size'].replace(replace_Size_values)\n",
    "# test_data['Outlet_Size'] = test_data['Outlet_Size'].replace(replace_Size_values)\n",
    "# replace_loc_values = {'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3}\n",
    "# data2['Outlet_Location_Type'] = data2['Outlet_Location_Type'].replace(replace_loc_values)\n",
    "# test_data['Outlet_Location_Type'] = test_data['Outlet_Location_Type'].replace(replace_loc_values)\n",
    "# data2.loc[data2['Item_Type'] == 'Household', 'Item_Fat_Content'] = 0\n",
    "# test_data.loc[test_data['Item_Type'] == 'Household', 'Item_Fat_Content'] = 0\n",
    "# data2['Item_Fat_Content'] = data2['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n",
    "# test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n",
    "\n",
    "# Handle missing values for all numerical columns\n",
    "numerical_cols_with_nan = data2.select_dtypes(include=np.number).columns[data2.select_dtypes(include=np.number).isnull().any()].tolist()\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "data2[numerical_cols_with_nan] = imputer_num.fit_transform(data2[numerical_cols_with_nan])\n",
    "test_data[numerical_cols_with_nan] = imputer_num.transform(test_data[numerical_cols_with_nan])\n",
    "data2['Item_Visibility'] = data2['Item_Visibility'].replace(0, data2['Item_Visibility'].mean())\n",
    "test_data['Item_Visibility'] = test_data['Item_Visibility'].replace(0, data2['Item_Visibility'].mean())\n",
    "\n",
    "train_ids = data2['Item_Identifier']\n",
    "outlet_train_ids = data2['Outlet_Identifier']\n",
    "test_ids = test_data['Item_Identifier']\n",
    "outlet_test_ids = test_data['Outlet_Identifier']\n",
    "\n",
    "data2 = data2.drop(['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1)\n",
    "test_data = test_data.drop(['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1)\n",
    "\n",
    "# Feature Selection\n",
    "selected_features = ['Item_Weight',  'Item_MRP', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Fat_Content', 'Outlet_Age']\n",
    "\n",
    "X = data2[selected_features]\n",
    "y = data2['Item_Outlet_Sales']\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "numerical_transformer = MinMaxScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(test_data[selected_features])\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV (faster than BayesSearchCV)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Reduced search space\n",
    "    'max_depth': [5, 10],  # Reduced search space\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)  # n_jobs=-1 for parallel processing\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=KFold(n_splits=3), scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_processed, y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_rmse = -grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters (Random Forest): {best_params}\")\n",
    "print(f\"Best RMSE (Cross-Validation): {best_rmse}\")\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# best_model = RandomForestRegressor(random_state=42, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], min_samples_leaf=best_params['min_samples_leaf'], n_jobs=-1)\n",
    "# best_model.fit(X_processed, y)\n",
    "\n",
    "# y_pred = best_model.predict(X_test_processed)\n",
    "\n",
    "# # Create Submission File\n",
    "# submission = pd.DataFrame({'Item_Identifier': test_ids, 'Outlet_Identifier': outlet_test_ids, 'Item_Outlet_Sales': y_pred})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Submission File\n",
    "# submission = pd.DataFrame({'Item_Identifier': test_ids, 'Outlet_Identifier': outlet_test_ids, 'Item_Outlet_Sales': y_pred})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# # print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
